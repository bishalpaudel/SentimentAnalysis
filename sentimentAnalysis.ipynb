{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentimentAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bishalpaudel/SentimentAnalysis/blob/master/sentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3XiUu7F5Eex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz2pOSU4v5yo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "1e3277d7-f0e1-48a0-e372-6500331ff1e8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KWnghw5wPsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(\"/content/drive/My Drive/AI/SentimentAnalysis/train.tsv\", delimiter=\"\\t\")\n",
        "df_test = pd.read_csv(\"/content/drive/My Drive/AI/SentimentAnalysis/test.tsv\", delimiter=\"\\t\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUICyoAi5k9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "review_train = df_train['Phrase']\n",
        "label_train = df_train['Sentiment']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hfbqRhV6sTg",
        "colab_type": "text"
      },
      "source": [
        "**2) Data Processing — convert to lower case**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH3C98_i6K4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "review_train = review_train.str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSzE4i7q7QOv",
        "colab_type": "text"
      },
      "source": [
        "**3) Data Processing — remove punctuation**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQA-I2ug7mdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "reviews_split = review_train.str.replace('[{}]'.format(string.punctuation), '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ca24QqP98kq",
        "colab_type": "text"
      },
      "source": [
        "**5) Tokenize — Create Vocab to Int mapping dictionary**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAlWiLiE-N24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "all_text2 = ' '.join(reviews_split);\n",
        "\n",
        "# Creates a list of words\n",
        "words = all_text2.split()\n",
        "\n",
        "# Make a word count list\n",
        "count_words = Counter(words)\n",
        "\n",
        "total_words = len(words)\n",
        "\n",
        "# Get the most used words\n",
        "sorted_words = count_words.most_common(total_words)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO-WJ9CtpfC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Why i + 1: In order to start from 1 index, and not 0.\n",
        "# Integer Encoding:\n",
        "vocab_to_int = {w:i+1 for i, (w, c) in enumerate(sorted_words)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKkz4CssqaNo",
        "colab_type": "text"
      },
      "source": [
        "**6) Tokenize — Encode the words:** Convert all the review text to integer based on integer-words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9bkZPObqpu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews_int = []\n",
        "for review in reviews_split:\n",
        "  r = [vocab_to_int[w] for w in review.split()]\n",
        "  reviews_int.append(r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJpkcA1ftjL0",
        "colab_type": "text"
      },
      "source": [
        "**7) Tokenize — Encode the labels**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9P4bz3qrD7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "encoded_labels = [1 if label>1 else 0 for label in label_train]\n",
        "encoded_labels = np.array(encoded_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-x1QEoctckS",
        "colab_type": "text"
      },
      "source": [
        "**8) Analyze Reviews Length**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCy-yjqotfhw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "c40ab993-6c69-4808-8a26-544d29135064"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "reviews_len = [len(x) for x in reviews_int]\n",
        "pd.Series(reviews_len).hist()\n",
        "plt.show()\n",
        "\n",
        "pd.Series(reviews_len).describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    156060.000000\n",
              "mean          6.716000\n",
              "std           6.435939\n",
              "min           0.000000\n",
              "25%           2.000000\n",
              "50%           4.000000\n",
              "75%           9.000000\n",
              "max          48.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMOVBJHBunix",
        "colab_type": "text"
      },
      "source": [
        "**9) Removing Outliers — Getting rid of extremely long or short reviews**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efgZWkOzuuW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews_int = [reviews_int[i] for i, l in enumerate(reviews_len) if l > 0]\n",
        "encoded_labels = [encoded_labels[i] for i, l in enumerate(reviews_len) if l > 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAb1B5gMg5q0",
        "colab_type": "text"
      },
      "source": [
        "**10) Padding / Truncating the remaining data**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wrt6qLIug5Zz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_features(reviews_int, seq_length):\n",
        "  features = np.zeros((len(reviews_int), seq_length), dtype=int)\n",
        "\n",
        "  for i, review in enumerate(reviews_int):\n",
        "    review_len = len(review)\n",
        "\n",
        "    # Pad with Zeros\n",
        "    if review_len < seq_length:\n",
        "      zeros = list(np.zeros(seq_length - review_len))\n",
        "      new = zeros + review\n",
        "    # Truncate\n",
        "    elif reviews_len > seq_length:\n",
        "      new = review[0:seq_length]\n",
        "\n",
        "    features[i,:] = np.array(new)\n",
        "  return features\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDo4FhfRjLD6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = pad_features(reviews_int, 200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4309iJEkU-E",
        "colab_type": "text"
      },
      "source": [
        "**11) Training, Validation, Test Dataset Split**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_4yJkRdkUvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "split_frac = 0.8\n",
        "len_feat = len(features)\n",
        "train_x = features[0 : int(split_frac * len_feat)]\n",
        "train_y = encoded_labels[0 : int(split_frac * len_feat)]\n",
        "\n",
        "remaining_x = features[int(split_frac * len_feat) : ]\n",
        "remaining_y = encoded_labels[int(split_frac * len_feat) : ]\n",
        " \n",
        "valid_x = remaining_x[0 : int(len(remaining_x) * 0.5)]\n",
        "valid_y = remaining_y[0 : int(len(remaining_y) * 0.5)]\n",
        "\n",
        "test_x = remaining_x[int(len(remaining_x) * 0.5) : ]\n",
        "test_y = remaining_y[int(len(remaining_y) * 0.5) : ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E3GnQjvyryD",
        "colab_type": "text"
      },
      "source": [
        "**12) Dataloaders and Batching**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7dLyoZgw5z8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(np.asarray(train_x)), torch.from_numpy(np.asarray(train_y)))\n",
        "valid_data = TensorDataset(torch.from_numpy(np.asarray(valid_x)), torch.from_numpy(np.asarray(valid_y)))\n",
        "test_data = TensorDataset(torch.from_numpy(np.asarray(test_x)), torch.from_numpy(np.asarray(test_y)))\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle = True, batch_size = batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle = True, batch_size = batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle = True, batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1092ihQyvIt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "85326acc-871e-46f2-9e09-cd663a07ddfa"
      },
      "source": [
        "train_on_gpu = True\n",
        "data_iter = iter(train_loader)\n",
        "sample_x, sample_y = data_iter.next()\n",
        "\n",
        "\n",
        "print('sample input size: ', sample_x.size())\n",
        "print('sample input \\n', sample_x)\n",
        "print()\n",
        "print('sample label size: ', sample_y.size())\n",
        "print('sample label \\n', sample_y)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample input size:  torch.Size([50, 200])\n",
            "sample input \n",
            " tensor([[   0,    0,    0,  ...,    2, 2882,  187],\n",
            "        [   0,    0,    0,  ...,    7,    1, 3839],\n",
            "        [   0,    0,    0,  ...,  133,  173,  116],\n",
            "        ...,\n",
            "        [   0,    0,    0,  ..., 4310,   14,  406],\n",
            "        [   0,    0,    0,  ...,    0,  370, 1921],\n",
            "        [   0,    0,    0,  ...,  145, 6547,  214]])\n",
            "\n",
            "sample label size:  torch.Size([50])\n",
            "sample label \n",
            " tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
            "        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
            "        1, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WiBsGh5ZVd_",
        "colab_type": "text"
      },
      "source": [
        "**14) Define the Model Class**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip8kExC8ZVGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SentimentLSTM(nn.Module):\n",
        "  def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "    super().__init__()\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    self.fc = nn.Linear(hidden_dim, output_size)\n",
        "    self.sig = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    batch_size = x.size(0)\n",
        "\n",
        "    # Passing x though embedding layer\n",
        "    embeds = self.embedding(x)\n",
        "\n",
        "    # Passing x though LSTM layer\n",
        "    lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "\n",
        "    # Stack up LSTM outputs\n",
        "    lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "\n",
        "    # Passing output though dropout layer: in order to regularize\n",
        "    out = self.dropout(lstm_out)\n",
        "\n",
        "    # Passing x though fully connected layer, in order to reduce dimension to linear\n",
        "    out = self.fc(out)\n",
        "\n",
        "    # Passing fully connected output through sigmoid in order to contain output between 0 and 1\n",
        "    sig_out = self.sig(out)\n",
        "\n",
        "    sig_out = sig_out.view(batch_size, -1)\n",
        "    sig_out = sig_out[:, -1]\n",
        "\n",
        "    return sig_out, hidden\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    ''' Initializes hidden state '''\n",
        "    weight = next(self.parameters()).data\n",
        "\n",
        "    if (train_on_gpu):\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "    else:\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "      \n",
        "    return hidden\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dyT16ltfp7-",
        "colab_type": "text"
      },
      "source": [
        "**15) Training the Network**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UjzTpOdf3Rm",
        "colab_type": "text"
      },
      "source": [
        "- **Instantiate the network**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqUl5O6Wfrqg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "5177f0ae-275c-40e8-ed16-fe0ce45e59a6"
      },
      "source": [
        "# +1 for 0 padding\n",
        "vocab_size = len(vocab_to_int) + 1\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "\n",
        "net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "print(net)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentLSTM(\n",
            "  (embedding): Embedding(16416, 400)\n",
            "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dj-Ol6AhM2S",
        "colab_type": "text"
      },
      "source": [
        "- **Training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Soz1I3iyhRGG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "d8a3d10c-f495-4a97-e930-a3da5705a510"
      },
      "source": [
        "lr = 0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "epochs = 4\n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip = 5\n",
        "\n",
        "if (train_on_gpu):\n",
        "  net.cuda()\n",
        "\n",
        "net.train()\n",
        "\n",
        "for e in range(epochs):\n",
        "  # Initialize hidden state\n",
        "  h = net.init_hidden(batch_size)\n",
        "\n",
        "  for inputs, labels in train_loader:\n",
        "    counter += 1;\n",
        "    if (inputs.size()[0] != 50):\n",
        "      continue\n",
        "    # if train_on_gpu:\n",
        "    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    net.zero_grad()\n",
        "\n",
        "    inputs = inputs.type(torch.LongTensor)\n",
        "    output, h = net(inputs, h)\n",
        "\n",
        "    # Calculate the loss and perform backprop\n",
        "    loss = criterion(output.squeeze(), labels.float())\n",
        "    loss.backward()\n",
        "\n",
        "    # `clip_grad_norm` helps prevent exploding gradient problem in RNNs/LSTMs\n",
        "    nn.utils.clip_grad_norm(net.parameters(), clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    if counter % print_every == 0:\n",
        "      # Get validation loss\n",
        "      val_h = net.init_hidden(batch_size)\n",
        "      val_losses = []\n",
        "      net.eval()\n",
        "\n",
        "      for inputs_v, labels_v in valid_loader:\n",
        "        if (inputs_v.size()[0] != 50):\n",
        "          continue\n",
        "\n",
        "        val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "        # if (train_on_gpu):\n",
        "        #   inputs_v, labels = inputs_v.cuda(), labels_v.cuda()\n",
        "\n",
        "        # print('inputs_v.size()')\n",
        "        # print(inputs_v.size()[0])\n",
        "        # print('inputs_v.size()[0] == 50')\n",
        "        # print(inputs_v.size()[0] == 50)\n",
        "        # print('val_h.count')\n",
        "        # print(val_h.count)\n",
        "        \n",
        "        inputs_v = inputs_v.type(torch.LongTensor)\n",
        "        output_valid, val_h = net(inputs_v, val_h)\n",
        "\n",
        "        val_loss = criterion(output_valid.squeeze(), labels_v.float())\n",
        "\n",
        "        val_losses.append(val_loss.item())\n",
        "      \n",
        "      net.train()\n",
        "      print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "            \"Step: {}...\".format(counter),\n",
        "            \"Loss: {:.6f}\".format(loss.item()),\n",
        "            \"Val loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-70aca9fffb4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Calculate the loss and perform backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-1d6a8467f309>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Passing x though embedding layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Passing x though LSTM layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m         return F.embedding(\n\u001b[1;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1812\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1814\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected object of device type cuda but got device type cpu for argument #3 'index' in call to _th_index_select"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDT7l70Et97i",
        "colab_type": "text"
      },
      "source": [
        "**16) Testing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9OmYRzxuAzf",
        "colab_type": "text"
      },
      "source": [
        "- **On Test Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VA5RiYIuKNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_losses = []\n",
        "num_correct = 0\n",
        "\n",
        "h = net.init_hidden(batch_size)\n",
        "\n",
        "net.eval()\n",
        "\n",
        "for inputs, labels in test_loader:\n",
        "  if (inputs.size()[0] != 50):\n",
        "    continue\n",
        "  h = tuple([each.data for each in h])\n",
        "\n",
        "  if(train_on_gpu):\n",
        "    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "  inputs = inputs.type(torch.LongTensor)\n",
        "  output, h = net(inputs, h)\n",
        "\n",
        "  test_loss = criterion(output.squeeze(), labels.float())\n",
        "  test_losses.append(test_loss.item())\n",
        "\n",
        "  # Convert output probabilities to predicted class (0 or 1)\n",
        "  pred = torch.round(output.squeeze())\n",
        "\n",
        "  # Compare predictions to true label\n",
        "  correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "  correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "  num_correct += np.sum(correct)\n",
        "\n",
        "# Average test loss\n",
        "print(\"Test Loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "# Accuracy over all test data\n",
        "test_acc = num_correct / len(test_loader.dataset)\n",
        "print(\"Test Accuracy: {:.3f}\".format(test_acc))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}